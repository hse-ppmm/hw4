{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья классификации\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "n = 250\n",
    "\n",
    "mu1 = np.array([0.0,0])\n",
    "mu2 = np.array([1.0,0])\n",
    "sigma1 = 5.0 * np.diag(np.array([1.0, 1.0]))\n",
    "sigma2 = 0.5 * np.diag(np.array([1.0, 1.0]))\n",
    "\n",
    "x1 = np.random.multivariate_normal(mu1, sigma1, n)\n",
    "x2 = np.random.multivariate_normal(mu2, sigma2, n)\n",
    "x = np.vstack([x1, x2])\n",
    "y = np.concatenate([np.full(x1.shape[0], 0), np.full(x2.shape[0], 1)])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(*x1.T,s=2.5)\n",
    "plt.scatter(*x2.T,s=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_dict(x):\n",
    "    if len(x) == 0:\n",
    "        return dict()\n",
    "    return {k: np.asarray([e[k] for e in x]) for k in x[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "max_depths = np.arange(1, 2 * np.log2(n))\n",
    "\n",
    "scoring = {\n",
    "    \"auc\":       \"roc_auc\",\n",
    "    \"accuracy\":  \"accuracy\",\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for max_depth in max_depths:\n",
    "    c = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    s = cross_validate(c, x, y.reshape(-1), cv=5, scoring=scoring, return_train_score=True)\n",
    "    scores.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, scores['train_accuracy'].mean(axis=1), '-*', label=\"train accuracy\")\n",
    "plt.plot(max_depths, scores['test_accuracy'].mean(axis=1), '-*', label=\"test accuracy\")\n",
    "plt.xlabel(\"Tree max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_grid = np.linspace(np.min(x), np.max(x), 2000)\n",
    "xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "xx_test = np.stack((xx,yy), axis=-1).reshape(-1, 2)\n",
    "\n",
    "c = DecisionTreeClassifier(random_state=0, max_depth=15)\n",
    "\n",
    "c.fit(x_train, y_train)\n",
    "pred = c.predict(xx_test).reshape(xx.shape)\n",
    "\n",
    "x1_train = x_train[y_train == 0]\n",
    "x2_train = x_train[y_train == 1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(-2,3)\n",
    "plt.ylim(-2,3)\n",
    "plt.contourf(xx, yy, pred, cmap=\"pink_r\")\n",
    "plt.scatter(*x1_train.T,s=2.5)\n",
    "plt.scatter(*x2_train.T,s=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "min_samples_leafs = np.arange(1, 20)\n",
    "\n",
    "scoring = {\n",
    "    \"auc\":       \"roc_auc\",\n",
    "    \"accuracy\":  \"accuracy\",\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for min_samples_leaf in min_samples_leafs:\n",
    "    c = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "    s = cross_validate(c, x, y.reshape(-1), cv=5, scoring=scoring, return_train_score=True)\n",
    "    scores.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(min_samples_leafs, scores['train_accuracy'].mean(axis=1), '-*', label=\"train accuracy\")\n",
    "plt.plot(min_samples_leafs, scores['test_accuracy'].mean(axis=1), '-*', label=\"test accuracy\")\n",
    "plt.xlabel(\"Leaf size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "path = c.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"Effective alpha\")\n",
    "_ = ax.set_ylabel(\"Total impurity of leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {\n",
    "    \"auc\":       \"roc_auc\",\n",
    "    \"accuracy\":  \"accuracy\",\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    c = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    s = cross_validate(c, x, y.reshape(-1), cv=5, scoring=scoring, return_train_score=True)\n",
    "    scores.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ccp_alphas, scores['train_accuracy'].mean(axis=1), '-*', label=\"train accuracy\")\n",
    "plt.plot(ccp_alphas, scores['test_accuracy'].mean(axis=1), '-*', label=\"test accuracy\")\n",
    "plt.xlabel(\"CCP alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья классификации своими руками\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from scipy import optimize\n",
    "\n",
    "Leaf = namedtuple('Leaf', ('value', 'x', 'y'))\n",
    "Node = namedtuple('Node', ('feature', 'value', 'impurity', 'left', 'right',))\n",
    "\n",
    "class BaseDecisionTree:\n",
    "    def __init__(self, x, y, max_depth=np.inf):\n",
    "        self.x = np.atleast_2d(x)\n",
    "        self.y = np.atleast_1d(y)\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.features = x.shape[1]\n",
    "        \n",
    "        self.root = self.build_tree(self.x, self.y)\n",
    "    \n",
    "    # Will fail in case of depth ~ 1000 because of limit of recursion calls\n",
    "    def build_tree(self, x, y, depth=1):\n",
    "        if depth > self.max_depth or self.criteria(y) < 1e-6:\n",
    "            return Leaf(self.leaf_value(y), x, y)\n",
    "        \n",
    "        feature, value, impurity = self.find_best_split(x, y)\n",
    "        \n",
    "        left_xy, right_xy = self.partition(x, y, feature, value)\n",
    "        left = self.build_tree(*left_xy, depth=depth + 1)\n",
    "        right = self.build_tree(*right_xy, depth=depth + 1)\n",
    "        \n",
    "        return Node(feature, value, impurity, left, right)\n",
    "    \n",
    "    def leaf_value(self, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def partition(self, x, y, feature, value):\n",
    "        i = x[:, feature] >= value\n",
    "        j = np.logical_not(i)\n",
    "        return (x[j], y[j]), (x[i], y[i])\n",
    "    \n",
    "    def _impurity_partition(self, value, feature, x, y):\n",
    "        (_, left), (_, right) = self.partition(x, y, feature, value)\n",
    "        return self.impurity(left, right)\n",
    "    \n",
    "    def find_best_split(self, x, y):\n",
    "        best_feature, best_value, best_impurity = 0, x[0,0], np.inf\n",
    "        for feature in range(self.features):\n",
    "            if x.shape[0] > 2:\n",
    "                x_interval = np.sort(x[:,feature])\n",
    "                res = optimize.minimize_scalar(\n",
    "                    self._impurity_partition, \n",
    "                    args=(feature, x, y),\n",
    "                    bounds=(x_interval[1], x_interval[-1]),\n",
    "                    method='Bounded',\n",
    "                )\n",
    "                assert res.success\n",
    "                value = res.x\n",
    "                impurity = res.fun\n",
    "            else:\n",
    "                value = np.max(x[:,feature])\n",
    "                impurity = self._impurity_partition(value, feature, x, y)\n",
    "            if impurity < best_impurity:\n",
    "                best_feature, best_value, best_impurity = feature, value, impurity\n",
    "        return best_feature, best_value, best_impurity\n",
    "    \n",
    "    # Can be optimized for given .criteria()\n",
    "    def impurity(self, left, right):\n",
    "        h_l = self.criteria(left)\n",
    "        h_r = self.criteria(right)\n",
    "        return (left.size * h_l + right.size * h_r) / (left.size + right.size)\n",
    "    \n",
    "    def criteria(self, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = np.atleast_2d(x)\n",
    "        y = np.empty(x.shape[0], dtype=self.y.dtype)\n",
    "        for i, row in enumerate(x):\n",
    "            node = self.root\n",
    "            while not isinstance(node, Leaf):\n",
    "                if row[node.feature] >= node.value:\n",
    "                    node = node.right\n",
    "                else:\n",
    "                    node = node.left\n",
    "            y[i] = node.value\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier(BaseDecisionTree):\n",
    "    def __init__(self, x, y, *args, random_state=None, **kwargs):\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        self.random_state = np.random.RandomState(random_state)\n",
    "        self.classes = np.unique(y)\n",
    "        super().__init__(x, y, *args, **kwargs)\n",
    "        \n",
    "    def leaf_value(self, y):\n",
    "        class_counts = np.sum(y == self.classes.reshape(-1,1), axis=1)\n",
    "        m = np.max(class_counts)\n",
    "        most_common = self.classes[class_counts == m]\n",
    "        if most_common.size == 1:\n",
    "            return most_common[0]\n",
    "        return self.random_state.choice(most_common)\n",
    "    \n",
    "    def criteria(self, y):\n",
    "        \"\"\"Gini\"\"\"\n",
    "        p = np.sum(y == self.classes.reshape(-1,1), axis=1) / y.size\n",
    "        return np.sum(p * (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MyDecisionTreeClassifier(x_train, y_train, max_depth=5, random_state=0)\n",
    "pred = c.predict(xx_test).reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = x_train[y_train == 0]\n",
    "x2_train = x_train[y_train == 1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(-2,3)\n",
    "plt.ylim(-2,3)\n",
    "plt.contourf(xx, yy, pred, cmap=\"pink_r\")\n",
    "plt.scatter(*x1_train.T,s=2.5)\n",
    "plt.scatter(*x2_train.T,s=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.1** Перед вами класс `BaseDecisionTree`. Используйте класс `MyDecisionTreeClassifier` в качестве примера, чтобы самостоятельно сделать простое дерево регрессии `MyDecisionTreeRegressor`.\n",
    "\n",
    "Для задачи регрессии в качестве значения листа уместно взять среднее арифметическое значений всех точек содержащихся в узле, в качестве критерия - среднеквадратичное отклонение.\n",
    "\n",
    "Используя синтетический пример с зависимостью (приведен ниже)\n",
    "$$y = 2 x + 1 + \\epsilon$$\n",
    "удебитесь в работоспособности дерева регрессии, подберите наилучший параметр `max_depth`.\n",
    "\n",
    "*Бонусные баллы* можно получить если реализовать функцию `impurity` более эффективно, упростив формулы для случая регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor(BaseDecisionTree):\n",
    "    def __init__(self, x, y, *args, random_state=None, **kwargs):\n",
    "        # код потерялся\n",
    "    \n",
    "    def leaf_value(self, y):\n",
    "        # код потерялся\n",
    "    \n",
    "    def criteria(self, y):\n",
    "        # код потерялся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 250\n",
    "x = np.random.uniform(0, 1, size=(n, 1))\n",
    "y = 2 * x[:, 0] + 1\n",
    "y = y + np.random.normal(0, 0.1, size=y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "dtr = MyDecisionTreeRegressor(x_train, y_train, random_state=0)\n",
    "\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')\n",
    "plt.plot(y_train, dtr.predict(x_train), 'o', label='train')\n",
    "plt.plot(y_test, dtr.predict(x_test), 'o', label='test')\n",
    "plt.legend()\n",
    "\n",
    "train_score = mean_squared_error(y_train, dtr.predict(x_train))\n",
    "test_score = mean_squared_error(y_test, dtr.predict(x_test))\n",
    "\n",
    "print(\"train score = {:.4f}\".format(train_score))\n",
    "print(\"test score = {:.4f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "n = 250\n",
    "\n",
    "mu1 = np.array([0.0,0])\n",
    "mu2 = np.array([1.0,0])\n",
    "sigma1 = 5.0 * np.diag(np.array([1.0, 1.0]))\n",
    "sigma2 = 0.5 * np.diag(np.array([1.0, 1.0]))\n",
    "\n",
    "x1 = np.random.multivariate_normal(mu1, sigma1, n)\n",
    "x2 = np.random.multivariate_normal(mu2, sigma2, n)\n",
    "x = np.vstack([x1, x2])\n",
    "y = np.concatenate([np.full(x1.shape[0], 0), np.full(x2.shape[0], 1)])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(*x1.T,s=2.5)\n",
    "plt.scatter(*x2.T,s=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "c = RandomForestClassifier(random_state=0, max_depth=15)\n",
    "\n",
    "c.fit(x_train, y_train)\n",
    "pred = c.predict(xx_test).reshape(xx.shape)\n",
    "\n",
    "x1_train = x_train[y_train == 0]\n",
    "x2_train = x_train[y_train == 1]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(-2,3)\n",
    "plt.ylim(-2,3)\n",
    "plt.contourf(xx, yy, pred, cmap=\"pink_r\")\n",
    "plt.scatter(*x1_train.T,s=2.5)\n",
    "plt.scatter(*x2_train.T,s=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.2** Используйте готовый класс `RandomForestClassifier` чтобы проверить как работает классификация с помощью метода случайного леса. С помощью метода кросс-валидации получите и постройте на графике зависимость точности (accuracy) (для учебного и тестового множеств) от максимальной глубины деревьев случайного леса. Используйте данные из примера про дерево классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код потерялся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, scores['train_accuracy'].mean(axis=1), '-*', label=\"train accuracy\")\n",
    "plt.plot(max_depths, scores['test_accuracy'].mean(axis=1), '-*', label=\"test accuracy\")\n",
    "plt.xlabel(\"Tree max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(c, x, y):\n",
    "    y_pred = c.predict(x)\n",
    "    if getattr(c, \"decision_function\", None):\n",
    "        scores = c.decision_function(x)\n",
    "    else:\n",
    "        scores = c.predict_proba(x)[:,1]\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y, y_pred, labels=['h', 'g']).ravel()\n",
    "    accuracy  = (tp + tn) / (tn + fp + fn + tp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall    = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    baccuracy = 0.5 * (specificity + recall)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print(\"Accuracy                  = {:.4f}\".format(accuracy))\n",
    "    print(\"Ballanced accuracy        = {:.4f}\".format(baccuracy))\n",
    "    print(\"F1                        = {:.4f}\".format(f1))\n",
    "    print(\"Precision (PPV)           = {:.4f}\".format(precision))\n",
    "    print(\"Recall (sensitivity, TPR) = {:.4f}\".format(recall))\n",
    "    print(\"Specificity (TNR, 1-FPR)  = {:.4f}\".format(specificity))\n",
    "    \n",
    "    min_score, max_score = np.min(scores), np.max(scores)\n",
    "    bins = np.linspace(min_score, max_score, 25)\n",
    "    plt.figure()\n",
    "    plt.hist(scores[y.reshape(-1) == 'h'], bins, alpha=0.5, label='Hadron (negative)')\n",
    "    plt.hist(scores[y.reshape(-1) == 'g'], bins, alpha=0.5, label='Gamma (positive)')\n",
    "    plt.xlabel(\"Decision function (value)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    \n",
    "    tpr, fpr, _ = sklearn.metrics.roc_curve(y, scores, pos_label='g')\n",
    "    auc = sklearn.metrics.roc_auc_score(y, scores)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    print(\"AUC                       = {:.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = [\"length\", \"width\", \"size\", \"conc\", \"conc1\", \"asym\", \"m3long\", \"m3trans\", \"alpha\", \"dist\", \"class\"]\n",
    "data = pd.read_csv('magic04.csv', names=names)\n",
    "\n",
    "x = np.asarray(data.iloc[:, :-1])\n",
    "y = np.asarray(data.iloc[:, [-1]])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "c = RandomForestClassifier(random_state=0)\n",
    "\n",
    "c.fit(x_train, y_train.reshape(-1))\n",
    "\n",
    "train_acc = c.score(x_train, y_train) # accuracy\n",
    "test_acc = c.score(x_test, y_test)\n",
    "\n",
    "evaluate(c, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.4**\n",
    "\n",
    "Используя данные с черенковского телескопа из файла `magic04.csv` и метод кросс-валидации подберите максимальную глубину случайного леса таким образом, чтобы получить наилучший AUC при бинарной классификации методом случайного леса (`RandomForestClassifier`).\n",
    "Постройте график зависимости AUC от глубины дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код потерялся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "print(\"fit time = {}\".format(scores['fit_time'].mean(axis=1)))\n",
    "for s in scoring.keys():\n",
    "    print(\"{} = {}\".format(s, scores[\"test_{}\".format(s)].mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, scores['test_auc'].mean(axis=1),'-*')\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Адаптивный бустинг\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "c = AdaBoostClassifier(random_state=0, n_estimators=200)\n",
    "\n",
    "c.fit(x_train, y_train)\n",
    "\n",
    "train_acc = c.score(x_train, y_train) # accuracy\n",
    "test_acc = c.score(x_test, y_test)\n",
    "\n",
    "evaluate(c, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.5**\n",
    "\n",
    "Используя данные с черенковского телескопа из файла `magic04.csv` и метод кросс-валидации подберите максимальную глубину дерева классификации, чтобы получить наилучший AUC при бинарной классификации методом адаптивного бустинга (`AdaBoostClassifier`).\n",
    "Постройте график зависимости AUC от глубины дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код потерялся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "print(\"fit time = {}\".format(scores['fit_time'].mean(axis=1)))\n",
    "for s in scoring.keys():\n",
    "    print(\"{} = {}\".format(s, scores[\"test_{}\".format(s)].mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, scores['test_auc'].mean(axis=1),'-*')\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "c = GradientBoostingClassifier(random_state=0, n_estimators=200)\n",
    "\n",
    "c.fit(x_train, y_train)\n",
    "\n",
    "train_acc = c.score(x_train, y_train) # accuracy\n",
    "test_acc = c.score(x_test, y_test)\n",
    "\n",
    "evaluate(c, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.6**\n",
    "\n",
    "Используя данные с черенковского телескопа из файла `magic04.csv` и метод кросс-валидации подберите максимальную глубину дерева классификации, чтобы получить наилучший AUC при бинарной классификации методом градиентного бустинга (`GradientBoostingClassifier`).\n",
    "Постройте график зависимости AUC от глубины дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код потерялся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flat_dict(scores)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "print(\"fit time = {}\".format(scores['fit_time'].mean(axis=1)))\n",
    "for s in scoring.keys():\n",
    "    print(\"{} = {}\".format(s, scores[\"test_{}\".format(s)].mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, scores['test_auc'].mean(axis=1),'-*')\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.7**\n",
    "\n",
    "За это задание можно получить *бонусные баллы*, за три наилучших решения среди всей группы.\n",
    "\n",
    "Используя один из четырех рассмотренных выше методов классификации с помощью деревьев, постройте бинарный классификатор для данных с черенковского телескопа из файла `magic04.csv` с наилучшим AUC для тестового набора данных. Можно подбирать любые гиперпараметры, которые окажется необходимо. Используйте функцию `evaluate`, чтобы нарисовать гистограмы классов и напечатать значения мер эффективности|."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изолирующий лес\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wl = np.asarray([7.8636, 8.0485, 8.2286, 8.4043, 8.5758, 8.7436, 8.9078, 9.0686, 9.2262, 9.3809, 9.5328, 9.6820, 9.8286, 9.9728, 10.1148, 10.2545, 10.3922, 10.5279, 10.6616, 10.7935, 10.9237, 11.0521, 11.1790, 11.3042, 11.4280, 11.5503, 11.6711, 11.7907, 11.9089, 12.0258, 12.1415, 12.2560, 12.3693, 12.4816, 12.5927, 12.7028, 12.8118, 12.9199, 13.0269, 13.1330, 13.2382, 13.3425, 13.4459, 13.5485, 10.9929, 11.3704, 11.7357, 12.0899, 12.4339, 12.7687, 13.0948, 13.4131, 13.7239, 14.0278, 14.3252, 14.6166, 14.9022, 15.1825, 15.4576, 15.7280, 15.9937, 16.2551, 16.5123, 16.7656, 17.0151, 17.2610, 17.5034, 17.7425, 17.9784, 18.2113, 18.4412, 18.6682, 18.8925, 19.1142, 19.3334, 19.5500, 19.7643, 19.9763, 20.1861, 20.3937, 20.5992, 20.8026, 21.0041, 21.2037, 21.4014, 21.5973, 21.7914, 21.9838, 22.1745, 22.3636, 22.5511, 22.7371, 22.9216])\n",
    "data = pd.read_csv('lrs.csv', header=None)\n",
    "\n",
    "x = np.asarray(data.iloc[:, 11:54])\n",
    "wl = wl[:x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wl, x[0,:], \"-\")\n",
    "plt.title(\"IRAS/LRS spectra\")\n",
    "plt.ylabel(\"Intensity (units)\")\n",
    "plt.xlabel(\"Wavelength (um)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "z = pca.fit_transform(x)\n",
    "\n",
    "c = IsolationForest(n_estimators=1000, contamination=\"auto\")\n",
    "c.fit(z)\n",
    "\n",
    "pred = c.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(*z.T, s=2.5, c=pred, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(wl, pca.components_[0,:], \"-\")\n",
    "plt.plot(wl, pca.components_[1,:], \"-\")\n",
    "plt.title(\"IRAS/LRS spectra\")\n",
    "plt.ylabel(\"Intensity (units)\")\n",
    "_ = plt.xlabel(\"Wavelength (um)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.8**\n",
    "\n",
    "Используйте данные спектров космических объектов с ИК спутника из файла `lsr.csv` и метод изолирующего леса `IsolationForest`, чтобы найти 10 наболее отличных от остальных спектров. Распечатайте соответсвтвующие таким спектрам строки из исходного объекта `data`. Нарисуйте на графике самый необычный спектр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# код потерялся"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
